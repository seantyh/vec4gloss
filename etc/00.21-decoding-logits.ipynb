{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "254163df-d703-433b-905b-b29ef6450d88",
   "metadata": {},
   "source": [
    "# Decoding Logits - Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95881a3-099e-4d42-af89-84af18de1184",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "if \"../src\" not in sys.path:\n",
    "    sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "051d63a6-dee0-45eb-941f-3de815f47dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from datasets import Dataset\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import MT5TokenizerFast\n",
    "from vec4gloss import check_hashes\n",
    "from vec4gloss import Vec4GlossModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dd263fde-f7bd-4761-ab1c-3770fc4f4108",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets.disable_caching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6667235-fb01-4c60-8900-2b10200bde10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_defgen = datasets.load_from_disk(\"../data/defgen_dataset_cwn\")\n",
    "vec4gloss_model_dir = \"../data/models/vec4gloss-defgen-220629-1250\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a042ea4-1339-42ee-843c-bbce498fb1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = MT5TokenizerFast.from_pretrained(vec4gloss_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37942b76-3827-4283-bb66-d7b161ee8780",
   "metadata": {},
   "source": [
    "## Preprocess functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa2bab9-b020-412b-8019-1bc1f83a3eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 256\n",
    "\n",
    "def get_marked_pos(text):\n",
    "    assert text.count(\"<\") == text.count(\">\") == 1\n",
    "    s, e = text.index(\"<\")+1, text.index(\">\")    \n",
    "    assert s != e\n",
    "    return s, e\n",
    "\n",
    "def add_marked_pos(ex):\n",
    "    pos = get_marked_pos(ex[\"src\"])\n",
    "    return {\"decoder_start_markers\": pos[0], \"decoder_end_markers\": pos[1]}\n",
    "\n",
    "def preprocess_fn(batch):    \n",
    "    src_batch = tokenizer(batch[\"src\"], \n",
    "                          max_length=max_length, truncation=True)\n",
    "    start_markers = [src_batch.char_to_token(bi,s) \n",
    "                     for bi, s in enumerate(batch[\"decoder_start_markers\"])]\n",
    "    end_markers = [src_batch.char_to_token(bi,e) \n",
    "                   for bi, e in enumerate(batch[\"decoder_end_markers\"])]\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        tgt_batch = tokenizer(batch[\"tgt\"],\n",
    "                              max_length=max_length, truncation=True)        \n",
    "        \n",
    "    return {\n",
    "        **src_batch, \n",
    "        \"decoder_start_markers\": start_markers,\n",
    "        \"decoder_end_markers\": end_markers,\n",
    "        \"labels\": tgt_batch[\"input_ids\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91bc6427-289b-4cd9-9048-6c3eae2a440e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at ../data/defgen_dataset_cwn/train\\cache-73645a22723c5115.arrow\n",
      "Loading cached processed dataset at ../data/defgen_dataset_cwn/test\\cache-79eff688f816c6ef.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1f3f6780e045fcbf428b7eff2ad4a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/77 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00f06f3a4a08450b98dcfde72e4f9c53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "drop_columns = [\"cwnid\", \"src\", \"tgt\"]\n",
    "ds_defgen = (ds_defgen.map(add_marked_pos)\n",
    "             .map(preprocess_fn, batched=True, remove_columns=drop_columns))\n",
    "train_ds = ds_defgen[\"train\"]\n",
    "test_ds = ds_defgen[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4db4fd9-90bf-4cac-b3e1-89e5d047e8e5",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af3ff1fd-dfb4-48ed-9144-642941bffc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "model = Vec4GlossModel.from_pretrained(vec4gloss_model_dir)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45b7f3a3-e819-461e-939f-c2430b2ce08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_cuda = torch.Generator()\n",
    "g_cuda.manual_seed(211321)\n",
    "loader = DataLoader(train_ds, batch_size=2, collate_fn=data_collator, shuffle=True, generator=g_cuda)\n",
    "batches = list(islice(loader, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5413304b-cad0-4b32-82b3-f15832500840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VC。模仿或照原樣重製他人的創意當作自己的。</s>',\n",
       " 'Nc。美術館的建築物及建築物所在的位置。</s><pad><pad><pad><pad><pad>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# labels\n",
    "batch = batches[0]\n",
    "tokenizer.batch_decode(torch.where(batch[\"labels\"]>=0, batch[\"labels\"], 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afe7de77-aed5-4e0e-bf52-e1b6d7f4a459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VC。比仿特定模原來的的製後的文字意。作自己的。</s>', 'Nc。美術館的建築物及建築物所在的位置。</s></s> N N N N']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## direct decoding settings\n",
    "with torch.no_grad():    \n",
    "    batch = batches[0]\n",
    "    out = model(**batch)\n",
    "tokenizer.batch_decode(out.logits.argmax(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b5dce11-b4aa-4510-86b6-4bc23d94b7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> VC。比喻自行取得資料。</s><pad><pad><pad><pad><pad>',\n",
       " '<pad> Nc。美術館的建築物及建築物所在的位置。</s>']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## generation setting\n",
    "dbg = {}\n",
    "batch = batches[0]\n",
    "gen_batch = {k:v for k, v in batch.items() if k not in (\"labels\", \"decoder_input_ids\")}\n",
    "tokenizer.batch_decode(model.generate(**gen_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e20bd319-14c9-41ae-9e05-190ed21f609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "genout = model.generate(**gen_batch, return_dict_in_generate=True, \n",
    "                        output_scores=True, output_hidden_states=True, \n",
    "                        output_attentions=True\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dde953b5-949f-40e9-a549-67ad4eb0979a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequences',\n",
       " 'scores',\n",
       " 'encoder_attentions',\n",
       " 'encoder_hidden_states',\n",
       " 'decoder_attentions',\n",
       " 'cross_attentions',\n",
       " 'decoder_hidden_states']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(genout.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7180d599-39f4-47e2-bc79-37d90838894d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VC。比喻自行取得資料。</s>字 VC VC VC VC', 'Nc。美術館的建築物及建築物所在的位置。</s>']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(torch.vstack([genout.scores[i].argmax(1) for i in range(len(genout.scores))]).permute(1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "851463b3-ce9d-4240-af2d-ef37917fb083",
   "metadata": {},
   "outputs": [],
   "source": [
    "intext = \"我們<上>山途中欣賞沿途風景。\"\n",
    "vbatch = tokenizer(intext, return_tensors=\"pt\")\n",
    "s,e = get_marked_pos(intext)\n",
    "s = vbatch.char_to_token(s)\n",
    "e = vbatch.char_to_token(e)\n",
    "vbatch[\"decoder_start_markers\"] = torch.tensor([s])\n",
    "vbatch[\"decoder_end_markers\"] = torch.tensor([e])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d13a5864-ab40-4714-b0e0-bdd9390da383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad> VCL。往高處移動或去。</s>']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vgenout = model.generate(**vbatch, max_length=30, \n",
    "                         return_dict_in_generate=True,\n",
    "                         output_scores=True, output_hidden_states=True, \n",
    "                        output_attentions=True)\n",
    "tokenizer.batch_decode(vgenout.sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83e07aed-a7d6-4e14-8718-839817713df5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sequences',\n",
       " 'scores',\n",
       " 'encoder_attentions',\n",
       " 'encoder_hidden_states',\n",
       " 'decoder_attentions',\n",
       " 'cross_attentions',\n",
       " 'decoder_hidden_states']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vgenout.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d0040a9a-5c5b-47c5-8831-5be2189c0c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## this is what I expect if decoding only uses a single vector\n",
    "## It's a tuple(gen length) of tuple (decoder layers) of tensor\n",
    "## (batch_size, num_heads, sequence_length, sequence_length)\n",
    "vgenout.cross_attentions[4][7].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec3efa-6845-452a-a467-5de51c9cc09b",
   "metadata": {},
   "source": [
    "## split the encoder decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "112f5933-46d0-47de-996e-6e2cfa863400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VCL。往高處移動或去。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_encoder_vector(intext, tokenizer, model):    \n",
    "    vbatch = tokenizer(intext, return_tensors=\"pt\")\n",
    "    s,e = get_marked_pos(intext)   \n",
    "    s = vbatch.char_to_token(s)\n",
    "    e = vbatch.char_to_token(e)\n",
    "    vbatch[\"decoder_start_markers\"] = torch.tensor([s])\n",
    "    vbatch[\"decoder_end_markers\"] = torch.tensor([e])\n",
    "    encoder = model.get_encoder()\n",
    "    enc_out = encoder(\n",
    "            input_ids=vbatch[\"input_ids\"], \n",
    "            attention_mask=vbatch[\"attention_mask\"])\n",
    "    enc_vec = enc_out.last_hidden_state[[0],s:e,:] \\\n",
    "                     .mean(1, keepdim=True)\n",
    "    return enc_vec\n",
    "\n",
    "def decode_vector(vec, tokenizer, model, max_length=50):\n",
    "    vgenout = model.generate(decoder_encoder_vector=vec, bos_token_id=0, max_length=max_length)\n",
    "    return tokenizer.batch_decode(vgenout[:, 1:-1])[0]\n",
    "enc_vec = extract_encoder_vector(\"我們<上>山途中欣賞沿途風景。\", tokenizer, model)\n",
    "decode_vector(enc_vec, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f2bf6b6f-4a61-4a54-8dca-0437ce205a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Na。燃燒後所發出的煙。'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_vec = extract_encoder_vector(\"在愛河邊施放五光十色的<煙火>\", tokenizer, model)\n",
    "decode_vector(enc_vec, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d1ea3e98-f2c7-4b8c-9dcd-8adb2be7a504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Na。燃燒後所產生的煙。'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_vec = extract_encoder_vector(\"在愛河邊施放五光十色的<煙>火\", tokenizer, model)\n",
    "decode_vector(enc_vec, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2b366fc-5650-480e-a11e-3c0c8faaccc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Na。火的火。'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_vec = extract_encoder_vector(\"在愛河邊施放五光十色的煙<火>\", tokenizer, model)\n",
    "decode_vector(enc_vec, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "808b4905-5417-4324-a2b9-0b0568b68cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'VH。形容光線充足且亮度高的。'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_vec = extract_encoder_vector(\"在愛河邊施放<五光十色>的煙火\", tokenizer, model)\n",
    "decode_vector(enc_vec, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e49d9d78-5a38-4a9f-8135-522f9b914e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nc。河川名,位於黃河注入湖海,為黃河注入湖海或其它河流的地方。'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_vec = extract_encoder_vector(\"在<愛河>邊施放五光十色的煙火\", tokenizer, model)\n",
    "decode_vector(enc_vec, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d087ff2-7f69-4cbc-8107-9780bc6a3e46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<CwnSense[03028901](明朗，VH): 形容光線充足明亮。>,\n",
       " <CwnSense[06649101](亮，VH): 形容光源的光線充足。>,\n",
       " <CwnSense[06685401](明，VH): 形容光源的光線充足。>,\n",
       " <CwnSense[07083901](朗，VH): 形容天空沒有烏雲遮蓋而光線充足明亮。>]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from CwnGraph import CwnImage\n",
    "cwn = CwnImage.latest()\n",
    "cwn.find_senses(definition=\"光線充足\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48123a66-1a0c-4da9-a35d-08534c0ad7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<CwnSense[05227708](爛，VH): 形容顏色亮度高的。>,\n",
       " <CwnSense[06539512](清，VH): 形容比喻顏色較淺且亮度高的。>,\n",
       " <CwnSense[06649110](亮，VH): 形容比喻顏色較淺且亮度高的。>,\n",
       " <CwnSense[06682803](鮮，VH): 形容顏色亮度高的。>,\n",
       " <CwnSense[06685417](明，VH): 形容比喻顏色較淺且亮度高的。>,\n",
       " <CwnSense[07060506](粉，VH): 形容顏色亮度高，彩度不飽合的。>,\n",
       " <CwnSense[07067408](嫩，VH): 形容顏色亮度高，彩度不飽合的。>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwn.find_senses(definition=\"亮度高\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62c59d7-80fd-4bd5-9568-7d49e93d512a",
   "metadata": {},
   "source": [
    "## Morphing vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9d0c54a-14d8-477d-bd49-c9b935e053f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 VCL。往高處移動或去。\n",
      "0.20 VCL。往高處移動。\n",
      "0.40 VCL。從參考位置的外面移到參考位置的裡面。\n",
      "0.60 VCL。從原來所在地經過一段行程到其他地點,通常是山。\n",
      "0.80 nom,VA。人們在約定俗成的固定時間內吃正餐。\n",
      "1.00 nom,VA。人們在約定俗成的固定時間內吃正餐。\n"
     ]
    }
   ],
   "source": [
    "enc_vec1 = extract_encoder_vector(\"我們<上>山途中欣賞沿途風景。\", tokenizer, model)\n",
    "enc_vec2 = extract_encoder_vector(\"我們在山裡<野餐>。\", tokenizer, model)\n",
    "delta = enc_vec2 - enc_vec1\n",
    "for i in np.arange(0, 1.01, 0.2):\n",
    "    print(f\"{i:.2f}\", decode_vector(enc_vec1+delta*i, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc2796a3-c351-4391-8646-fefab409168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 D。表突然出現在動詞之後。\n",
      "0.20 D。表突然出現在腦海中。\n",
      "0.40 D。表事件發生的頻率比預期低。\n",
      "0.60 VH。形容比喻特定事件沒有被發生過。\n",
      "0.80 VH。形容比喻特定對象沒有被使用。\n",
      "1.00 VH。形容比喻特定版面變成沒有被填滿的狀態。\n"
     ]
    }
   ],
   "source": [
    "enc_vec1 = extract_encoder_vector(\"我們上山時，天<突然>下起了大雪。\", tokenizer, model)\n",
    "enc_vec2 = extract_encoder_vector(\"為什麼我的留言板是<空>的？\", tokenizer, model)\n",
    "delta = enc_vec2 - enc_vec1\n",
    "for i in np.arange(0, 1.01, 0.2):\n",
    "    print(f\"{i:.2f}\", decode_vector(enc_vec1+delta*i, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0718447e-ebf5-4cd6-a1fd-d93b4fd7a177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 Na。植物的主要器官之一,主要用於繁殖,通常具有顏色鮮豔和形狀漂亮的花瓣。\n",
      "0.20 Na。植物名,薔薇科櫻屬,多年生草本,葉呈長橢圓形,有花瓣五片,有粉紅、白、紅等顏色。\n",
      "0.40 Na。植物名,薔薇科櫻屬,多年生草本,葉呈長橢圓形,有花瓣五片,有粉紅、白、紅等顏色。\n",
      "0.60 Na。書寫筆畫的一種,由左向右上斜的筆畫。\n",
      "0.80 Na。筆畫的一種,由左向右上斜的筆畫。\n",
      "1.00 Nf。計算筆畫的單位。\n"
     ]
    }
   ],
   "source": [
    "enc_vec1 = extract_encoder_vector(\"那是一位嬌豔如<花>的少女。\", tokenizer, model)\n",
    "enc_vec2 = extract_encoder_vector(\"以動畫方式慢速地顯示字母的每一<筆>一劃。\", tokenizer, model)\n",
    "delta = enc_vec2 - enc_vec1\n",
    "for i in np.arange(0, 1.01, 0.2):\n",
    "    print(f\"{i:.2f}\", decode_vector(enc_vec1+delta*i, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4539c736-4217-4078-bd5a-399dbb582728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 Na。哺乳類動物,瞳孔會因光線強弱而變大小,行動敏捷。\n",
      "0.20 Na。以貓為形象製成的人造物。\n",
      "0.40 Na。以貓為形象製成的人造物。\n",
      "0.60 VC。比喻以不正當的方式取得財物。\n",
      "0.80 VC。比喻以不正當的方式取得財物。\n",
      "1.00 VC。調查或發現不合規範的行為。\n"
     ]
    }
   ],
   "source": [
    "enc_vec1 = extract_encoder_vector(\"昨天我的<貓>抓了三隻老鼠。\", tokenizer, model)\n",
    "enc_vec2 = extract_encoder_vector(\"昨天我的貓<抓>了三隻老鼠。\", tokenizer, model)\n",
    "delta = enc_vec2 - enc_vec1\n",
    "for i in np.arange(0, 1.01, 0.2):\n",
    "    print(f\"{i:.2f}\", decode_vector(enc_vec1+delta*i, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1895b90-2dc4-44c8-a607-e5b1fe29b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 Nb。外文名字。\n",
      "0.20 Nb。外文名字。\n",
      "0.40 Nb。美國私立大學之一,位於美國東岸,為美國經濟、工商業中心。\n",
      "0.60 Nb。美國私立大學之一,位於美國東岸,為美國經濟、工商業中心。\n",
      "0.80 Na。伊拉克的文化。\n",
      "1.00 Na。伊拉克的文化。\n"
     ]
    }
   ],
   "source": [
    "enc_vec1 = extract_encoder_vector(\"<布丁狗>是三麗鷗今年票選第一名的角色。\", tokenizer, model)\n",
    "enc_vec2 = extract_encoder_vector(\"<凱蒂貓>有五個蘋果高三個蘋果重。\", tokenizer, model)\n",
    "delta = enc_vec2 - enc_vec1\n",
    "for i in np.arange(0, 1.01, 0.2):\n",
    "    print(f\"{i:.2f}\", decode_vector(enc_vec1+delta*i, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ecdb20b0-7d2d-4d9b-aa66-a3cb64bab0ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00 Na。中華民族的主要通用語文,為中國大陸、臺灣、新加坡的官方語言。\n",
      "0.20 Na。中華人民共和國的語文學系統,由中國國民黨創立。\n",
      "0.40 Na。臺灣國立大學之一,位於臺北市文山區。\n",
      "0.60 Nb。臺灣國立大學之一,位於臺北市文山區。\n",
      "0.80 Nb。臺灣國立大學之一,位於臺北市大安區。\n",
      "1.00 Nb。臺灣國立大學之一,位於臺北市大安區。\n"
     ]
    }
   ],
   "source": [
    "enc_vec1 = extract_encoder_vector(\"<中文詞彙網路>是台大語言所維護的語言資源之一。\", tokenizer, model)\n",
    "enc_vec2 = extract_encoder_vector(\"中文詞彙網路是<台大語言所>維護的語言資源之一。\", tokenizer, model)\n",
    "delta = enc_vec2 - enc_vec1\n",
    "for i in np.arange(0, 1.01, 0.2):\n",
    "    print(f\"{i:.2f}\", decode_vector(enc_vec1+delta*i, tokenizer, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9550aed2-f492-479b-b313-63f45f7233dd",
   "metadata": {},
   "source": [
    "## More playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0497dde7-c3a8-49e7-9dc3-7b312b833e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_func(tokenizer, model):\n",
    "    def _gen_func(text):\n",
    "        enc_vec = extract_encoder_vector(text, tokenizer, model)\n",
    "        return decode_vector(enc_vec, tokenizer, model)\n",
    "    return _gen_func\n",
    "gen = gen_func(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "43e03609-21bb-481d-a05d-e0763edec5ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Na。指由專業人員編排並提供授予碩士或博士學位的教育系統。'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen(\"<中文詞彙網路是台大語言所維護的語言資源之一。>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d7857c41-1b58-4762-a209-5842e1d053b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nom,VC。在服裝或服飾裝飾中,依照其美麗外貌,改變外表造型。\n",
      "VC。以特定形式做出造型。\n",
      "nom,VA。依特定對象的造型或頭部做造型。\n",
      "nom,VA。依照特定的舉例做造型。\n",
      "nom,VA。依照特定材料做造型以進行特定活動。\n",
      "nom,VA。依照特定造型或造型更改造型。\n",
      "nom,VC。改變造型或外表造型。\n",
      "nom,VC。依照特定的形狀或立場將特定對象的性質或狀態改變。\n",
      "nom,VA。改變造型。\n",
      "nom,VC。在正式場合或特定活動中擔任特定角色。\n"
     ]
    }
   ],
   "source": [
    "enc_vec = extract_encoder_vector(\"在台大校慶典禮上，學生<扮裝>進場。。\", tokenizer, model)\n",
    "for _ in range(10):\n",
    "    print(decode_vector(enc_vec+torch.randn(768)*0.05, tokenizer, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c591d847-ab94-44fe-8da2-475b0c31564c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
